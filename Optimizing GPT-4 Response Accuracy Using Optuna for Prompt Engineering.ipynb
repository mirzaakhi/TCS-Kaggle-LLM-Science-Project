{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff5afa-12c5-4493-b9bb-a0e6916c3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import optuna\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set up OpenAI API key from environment variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up logging for Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.DEBUG)\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('/Users/akhi/Desktop/TCS Kaggle Project/train.csv')\n",
    "test_df = pd.read_csv('/Users/akhi/Desktop/TCS Kaggle Project/test.csv')\n",
    "\n",
    "# Limit the dataset for optimization\n",
    "train_df = train_df.sample(n=100, random_state=42)  # Using a smaller subset for quicker optimization\n",
    "\n",
    "# Combine question and options into a single prompt\n",
    "def create_prompt(row):\n",
    "    return (f\"Question: {row['prompt']}\\n\"\n",
    "            f\"A) {row['A']}\\n\"\n",
    "            f\"B) {row['B']}\\n\"\n",
    "            f\"C) {row['C']}\\n\"\n",
    "            f\"D) {row['D']}\\n\"\n",
    "            f\"E) {row['E']}\\n\\n\"\n",
    "            f\"Please provide the letter of the correct option. Only provide the letter.\")\n",
    "\n",
    "train_df['prompt_combined'] = train_df.apply(create_prompt, axis=1)\n",
    "queries = train_df['prompt_combined'].tolist()\n",
    "true_answers = train_df['answer'].tolist()\n",
    "\n",
    "def generate_response(prompt, temperature):\n",
    "    for _ in range(5):  # Retry up to 5 times\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question with only the letter of the correct option.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,  # Use the temperature parameter from Optuna\n",
    "                max_tokens=10  # Limit the response length to avoid additional text\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except openai.error.RateLimitError as e:\n",
    "            print(f\"Rate limit error: {e}. Retrying...\")\n",
    "            time.sleep(random.uniform(0.5, 2))  # Wait for a random time between 0.5 to 2 seconds before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"\"\n",
    "    return \"\"  # Return an empty string if all retries fail\n",
    "\n",
    "def extract_letter(response):\n",
    "    # Extract the first uppercase letter from the response\n",
    "    for char in response:\n",
    "        if char in ['A', 'B', 'C', 'D', 'E']:\n",
    "            return char\n",
    "    return \"Invalid\"  # Return \"Invalid\" if no letter is found\n",
    "\n",
    "def evaluate_accuracy(temperature):\n",
    "    results = []\n",
    "    correct = 0\n",
    "    \n",
    "    for i, prompt in enumerate(queries):\n",
    "        response = generate_response(prompt, temperature)\n",
    "        predicted_answer = extract_letter(response)\n",
    "        results.append(predicted_answer)\n",
    "        if predicted_answer == true_answers[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(true_answers)\n",
    "    return accuracy\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest a temperature parameter to Optuna\n",
    "    temperature = trial.suggest_float('temperature', 0.0, 1.0)\n",
    "    \n",
    "    # Evaluate accuracy with the suggested temperature\n",
    "    print(f\"Starting trial with temperature: {temperature}\")\n",
    "    accuracy = evaluate_accuracy(temperature)\n",
    "    print(f\"Finished trial with temperature: {temperature}, accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "print(\"Creating Optuna study...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "print(\"Starting optimization...\")\n",
    "study.optimize(objective, n_trials=5)  # Reduced number of trials for quicker optimization\n",
    "\n",
    "best_temperature = study.best_params['temperature']\n",
    "print(f\"Best Temperature: {best_temperature}\")\n",
    "\n",
    "# Evaluate the model with the best temperature found by Optuna\n",
    "results = []\n",
    "correct = 0\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "for i, prompt in enumerate(queries):\n",
    "    response = generate_response(prompt, best_temperature)\n",
    "    predicted_answer = extract_letter(response)\n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"True Answer\": true_answers[i],\n",
    "        \"Predicted Answer\": predicted_answer\n",
    "    })\n",
    "\n",
    "    # Print a sample of the prompts, true answers, and predicted answers for debugging\n",
    "    if i < 5:  # Adjust this number to print more samples if needed\n",
    "        print(f\"Sample {i + 1}\")\n",
    "        print(f\"Prompt:\\n{prompt}\")\n",
    "        print(f\"True Answer: {true_answers[i]}\")\n",
    "        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "        print()\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for true, pred in zip(true_answers, results_df['Predicted Answer']) if true.strip().upper() == pred.strip().upper()])\n",
    "accuracy = correct / len(true_answers)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "\n",
    "# Display the full dataset with results\n",
    "print(results_df)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_answers, results_df['Predicted Answer'], zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_answers, results_df['Predicted Answer'], labels=list(set(true_answers)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(true_answers)))\n",
    "disp.plot(cmap='viridis')\n",
    "plt.title('Confusion Matrix - Prompt Engineering')\n",
    "plt.savefig('confusion_matrix_promptengineering.png')  # Save the confusion matrix plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter_env)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
